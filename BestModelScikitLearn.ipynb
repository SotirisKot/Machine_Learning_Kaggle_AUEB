{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation, Imports and Function Declarations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install GGPLOT\n",
    "!python -m pip install ggplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import geopy.distance\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from ggplot import *\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "def get_distance_km(lat1, lon1, lat2, lon2):\n",
    "    return geopy.distance.distance((lat1, lon1), (lat2, lon2)).km\n",
    "\n",
    "import datetime\n",
    "\n",
    "def transform_date(date):\n",
    "    dates = date.split('-')\n",
    "    datef = datetime.datetime(int(dates[0]),int(dates[1]),int(dates[2]))\n",
    "    return datef.year, datef.month, datef.day, datef.weekday()\n",
    "\n",
    "def Holiday(month, day):\n",
    "    if month == 7 and day <= 10: return 'IDD'\n",
    "    if month == 12: return 'CRI'\n",
    "    if month in [3,4]: return 'SRB'\n",
    "    if month == 11 and day >=22 and day<=28: return 'THG'\n",
    "    if month == 1: return 'NYR'\n",
    "    return 'NOT'\n",
    "\n",
    "def Season(month,day):\n",
    "    if(month in [9,10,11]) : return 'AUT'\n",
    "    if(month in [12,1,2]) : return 'WIN'\n",
    "    if(month in [3,4,5]) : return 'SPR'\n",
    "    if(month in [6,7,8]) : return 'SUM'\n",
    "    return 'NOT'\n",
    "\n",
    "def train(development):\n",
    "    df_train = pd.read_csv('dataset/train.csv')\n",
    "    y_train = df_train[['PAX']]\n",
    "    y_test = None\n",
    "    \n",
    "    if(development==False):\n",
    "        df_test = pd.read_csv('dataset/test.csv')\n",
    "    else:\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        df_train, df_test, y_train, y_test = train_test_split(df_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "    ### Extract the date and add new features from date\n",
    "\n",
    "    # TRAIN SET\n",
    "    tfdates = df_train.apply(lambda row: transform_date(row['DateOfDeparture']), axis=1)\n",
    "    years = [t[0] for t in tfdates]\n",
    "    months = [t[1] for t in tfdates]\n",
    "    days = [t[2] for t in tfdates]\n",
    "    weekdays = [t[3] for t in tfdates]\n",
    "    df_train['Year'], df_train['Month'],df_train['Day'], df_train['WeekDay'] = years, months, days, weekdays\n",
    "\n",
    "    # TEST SET\n",
    "    tfdates = df_test.apply(lambda row: transform_date(row['DateOfDeparture']), axis=1)\n",
    "    years = [t[0] for t in tfdates]\n",
    "    months = [t[1] for t in tfdates]\n",
    "    days = [t[2] for t in tfdates]\n",
    "    weekdays = [t[3] for t in tfdates]\n",
    "    df_test['Year'], df_test['Month'],df_test['Day'], df_test['WeekDay'] = years, months, days, weekdays\n",
    "\n",
    "    ### Extract the distance from coordinates, longtitude and latitude are inversed -- !!!Dataset's error!!!\n",
    "\n",
    "    # TRAIN SET\n",
    "    distances = df_train.apply(lambda row: round(get_distance_km(row['LongitudeDeparture'],row['LatitudeDeparture'],row['LongitudeArrival'],row['LatitudeArrival']),3), axis=1)\n",
    "    df_train['Distance'] = distances\n",
    "\n",
    "    # TEST SET\n",
    "    distances = df_test.apply(lambda row: round(get_distance_km(row['LongitudeDeparture'],row['LatitudeDeparture'],row['LongitudeArrival'],row['LatitudeArrival']),3), axis=1)\n",
    "    df_test['Distance'] = distances\n",
    "\n",
    "    ### Set min and max weeks to departure\n",
    "\n",
    "    # TRAIN SET\n",
    "    mins = df_train.apply(lambda row: round(row['WeeksToDeparture']-row['std_wtd'],3), axis=1)\n",
    "    maxs = df_train.apply(lambda row: round(row['WeeksToDeparture']+row['std_wtd'],3), axis=1)\n",
    "\n",
    "    df_train['MinWTD'] = mins\n",
    "    df_train['MaxWTD'] = maxs\n",
    "\n",
    "    # TEST SET\n",
    "    mins = df_test.apply(lambda row: round(row['WeeksToDeparture']-row['std_wtd'],3), axis=1)\n",
    "    maxs = df_test.apply(lambda row: round(row['WeeksToDeparture']+row['std_wtd'],3), axis=1)\n",
    "\n",
    "    df_test['MinWTD'] = mins\n",
    "    df_test['MaxWTD'] = maxs\n",
    "\n",
    "    ### Find holidays, seasons\n",
    "\n",
    "    # TRAIN SET\n",
    "    holis = df_train.apply(lambda row: Holiday(row['Month'],row['Day']), axis=1)\n",
    "    seas = df_train.apply(lambda row: Season(row['Month'],row['Day']), axis=1)\n",
    "\n",
    "    df_train['Holiday'] = holis\n",
    "    df_train['Season'] = seas\n",
    "\n",
    "    # TEST SET\n",
    "    holis = df_test.apply(lambda row: Holiday(row['Month'],row['Day']), axis=1)\n",
    "    seas = df_test.apply(lambda row: Season(row['Month'],row['Day']), axis=1)\n",
    "\n",
    "    df_test['Holiday'] = holis\n",
    "    df_test['Season'] = seas\n",
    "\n",
    "    torem = ['DateOfDeparture','CityDeparture','LongitudeDeparture','LatitudeDeparture','CityArrival','LongitudeArrival','LatitudeArrival','WeeksToDeparture','std_wtd','PAX','MinWTD','MaxWTD']\n",
    "    \n",
    "    if(development==False):\n",
    "        \n",
    "        df_train.drop(torem, axis=1, inplace=True)\n",
    "                \n",
    "        torem.remove('PAX')\n",
    "        \n",
    "        df_test.drop(torem, axis=1, inplace=True)\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        df_train.drop(torem, axis=1, inplace=True)\n",
    "        \n",
    "        df_test.drop(torem, axis=1, inplace=True)\n",
    "        \n",
    "    df_train.reset_index(drop=True,inplace=True)\n",
    "    df_test.reset_index(drop=True,inplace=True)\n",
    "        \n",
    "    print(df_train.head(),'\\n'*5)\n",
    "    print(df_test.head())\n",
    "    \n",
    "    return df_train, df_test, y_train, y_test\n",
    "\n",
    "def runModel(df_train, df_test, y_train, showTsne):\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    \n",
    "    le = LabelEncoder()\n",
    "    # Departure and Arrival have the same values so we train only on Departure\n",
    "    le.fit(df_train['Departure'])\n",
    "    df_train['Departure'] = le.transform(df_train['Departure'])\n",
    "    df_train['Arrival'] = le.transform(df_train['Arrival'])\n",
    "    \n",
    "    df_test['Departure'] = le.transform(df_test['Departure'])\n",
    "    df_test['Arrival'] = le.transform(df_test['Arrival'])\n",
    "\n",
    "    le.fit(df_train['Holiday'])\n",
    "    df_train['Holiday'] = le.transform(df_train['Holiday'])\n",
    "    le.fit(df_test['Holiday'])\n",
    "    df_test['Holiday'] = le.transform(df_test['Holiday'])\n",
    "\n",
    "    le.fit(df_train['Season'])\n",
    "    df_train['Season'] = le.transform(df_train['Season'])\n",
    "    le.fit(df_test['Season'])\n",
    "    df_test['Season'] = le.transform(df_test['Season'])\n",
    "\n",
    "    from sklearn.ensemble import AdaBoostClassifier\n",
    "    from sklearn.ensemble import GradientBoostingClassifier\n",
    "    from sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier\n",
    "    from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "    import numpy as np\n",
    "    import codecs\n",
    "\n",
    "    X_train = df_train\n",
    "    X_test = df_test\n",
    "    y_train = np.ravel(y_train)\n",
    "\n",
    "    ### Scale the data\n",
    "    \n",
    "    from sklearn.preprocessing import minmax_scale\n",
    "    \n",
    "    X_train['Distance'] = minmax_scale(X_train['Distance'])\n",
    "    X_test['Distance'] = minmax_scale(X_test['Distance'])\n",
    "    \n",
    "    # # One-Hot encoding\n",
    "\n",
    "    X_train_dep = pd.get_dummies(X_train['Departure'],prefix='dep')\n",
    "    X_train_arr = pd.get_dummies(X_train['Arrival'],prefix='arr')\n",
    "\n",
    "    cols = X_train.columns[[0,1,2,3,4,5,7,8]]\n",
    "    X_train[cols] = minmax_scale(X_train[cols])\n",
    "        \n",
    "    X_train_extra = pd.concat([X_train['Departure'], X_train['Arrival'], X_train['Year'], X_train['Month'], X_train['Day'], X_train['WeekDay'], X_train['Holiday'], X_train['Season'], X_train['Distance']],axis=1)\n",
    "    \n",
    "    from sklearn.decomposition import PCA\n",
    "    \n",
    "    pca = PCA(n_components=1)\n",
    "    pca.fit(X_train_extra)\n",
    "    X_train_extra = pca.transform(X_train_extra)\n",
    "    X_train_extra = pd.DataFrame(X_train_extra)\n",
    "    \n",
    "    X_train_extra.reset_index(drop=True,inplace=True)\n",
    "       \n",
    "    X_train = pd.concat([X_train,X_train_extra,X_train_dep,X_train_arr],axis=1,ignore_index=True)\n",
    "    \n",
    "    X_train.drop([0,1],axis=1, inplace=True)\n",
    "    \n",
    "    idx_scale = [9]\n",
    "    for i in idx_scale:\n",
    "        X_train[i] = minmax_scale(X_train[i])\n",
    "    \n",
    "    \n",
    "    X_test_dep = pd.get_dummies(X_test['Departure'],prefix='dep')\n",
    "    X_test_arr = pd.get_dummies(X_test['Arrival'],prefix='arr')\n",
    "    \n",
    "    cols = X_test.columns[[0,1,2,3,4,5,7,8]]\n",
    "    X_test[cols] = minmax_scale(X_test[cols])\n",
    "        \n",
    "    X_test_extra = pd.concat([X_test['Departure'], X_test['Arrival'], X_test['Year'], X_test['Month'], X_test['Day'], X_test['WeekDay'], X_test['Holiday'], X_test['Season'], X_test['Distance']],axis=1)\n",
    "    \n",
    "    X_test_extra = pca.transform(X_test_extra)\n",
    "    X_test_extra = pd.DataFrame(X_test_extra)\n",
    "    \n",
    "    X_test_extra.reset_index(drop=True,inplace=True)\n",
    "    \n",
    "    X_test = pd.concat([X_test,X_test_extra,X_test_dep,X_test_arr],axis=1, ignore_index=True)\n",
    "    \n",
    "    X_test.drop([0,1],axis=1, inplace=True)\n",
    "    \n",
    "    idx_scale = [9]\n",
    "    \n",
    "    for i in idx_scale:\n",
    "        X_test[i] = minmax_scale(X_test[i])\n",
    "    \n",
    "\n",
    "    ## Print the data\n",
    "    \n",
    "    print(X_train.head())\n",
    "    \n",
    "    ## TSNE\n",
    "    \n",
    "    if(showTsne):\n",
    "    \n",
    "        from sklearn.manifold import TSNE \n",
    "        tsne = TSNE(n_components=2,n_iter=250)\n",
    "        tsne_res = tsne.fit_transform(X_train)\n",
    "\n",
    "        df_tnse = pd.DataFrame(tsne_res)\n",
    "\n",
    "        df_pax = pd.DataFrame(y_train)\n",
    "\n",
    "        df_tnse = pd.concat([df_tnse,df_pax],axis=1,ignore_index=True)\n",
    "\n",
    "        df_tnse.columns = ['X','Y','label']\n",
    "\n",
    "        df_tnse['label'] = df_tnse['label'].astype(str)\n",
    "\n",
    "        print(df_tnse.head())\n",
    "\n",
    "        chart = ggplot(df_tnse,aes(x='X',y='Y',color='label'))+ geom_point(alpha=0.8) + ggtitle('tSNE')\n",
    "\n",
    "        chart.show()\n",
    "    \n",
    "    clf1 = AdaBoostClassifier(\n",
    "        ExtraTreeClassifier(max_depth=10),\n",
    "        n_estimators=500,\n",
    "        learning_rate=0.01,\n",
    "        algorithm=\"SAMME\")\n",
    "    clf2 = GradientBoostingClassifier(\n",
    "         n_estimators=400,\n",
    "         learning_rate=0.1,\n",
    "         max_depth=3,\n",
    "         subsample=1)\n",
    "    clf3 = RandomForestClassifier(n_estimators=600,max_depth=None,criterion='entropy')\n",
    "\n",
    "    from sklearn.multiclass import OneVsOneClassifier\n",
    "    \n",
    "    clf = OneVsOneClassifier(VotingClassifier(estimators=[('ada',clf1),('gb',clf2),('rf',clf3)],voting='soft',weights=[1,1.5,2]))\n",
    "    \n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    print(X_train.shape)\n",
    "    print(X_test.shape)\n",
    "    \n",
    "    return y_pred\n",
    "    \n",
    "def evaluateModel(y_pred, y_test, development):\n",
    "    if(development==False):\n",
    "        # Write the predictions to the file\n",
    "        import csv\n",
    "        with open('y_pred.csv', 'w', newline='') as csvfile:\n",
    "            writer = csv.writer(csvfile, delimiter=',')\n",
    "            writer.writerow(['Id', 'Label'])\n",
    "            for i in range(y_pred.shape[0]):\n",
    "                writer.writerow([i, y_pred[i]])\n",
    "        return None\n",
    "    else:\n",
    "        # FOR DEVELOPMENT SET\n",
    "        print(y_pred)\n",
    "        from sklearn.metrics import f1_score\n",
    "        return f1_score(y_test, y_pred, average='micro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "development = False\n",
    "\n",
    "df_train, df_test, y_train, y_test = train(development)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showTsne = False\n",
    "y_pred = runModel(df_train, df_test, y_train, showTsne)\n",
    "score = evaluateModel(y_pred, y_test, development)\n",
    "if(score != None): print(score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
