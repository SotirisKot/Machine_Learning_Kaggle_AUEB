{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation, Imports and Function Declarations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TmQfE14CaoNq"
   },
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s30b8-qgTMWu"
   },
   "outputs": [],
   "source": [
    "# Install GGPLOT\n",
    "!python -m pip install ggplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ughp4xPQaoNv"
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import geopy.distance\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from ggplot import *\n",
    "\n",
    "def get_distance_km(lat1, lon1, lat2, lon2):\n",
    "    return geopy.distance.distance((lat1, lon1), (lat2, lon2)).km\n",
    "\n",
    "import datetime\n",
    "\n",
    "def transform_date(date):\n",
    "    dates = date.split('-')\n",
    "    datef = datetime.datetime(int(dates[0]),int(dates[1]),int(dates[2]))\n",
    "    return datef.year, datef.month, datef.day, datef.weekday()\n",
    "\n",
    "def Holiday(month, day):\n",
    "    if month == 7 and day <= 10: return 'IDD'\n",
    "    if month == 12: return 'CRI'\n",
    "    if month in [3,4]: return 'SRB'\n",
    "    if month == 11 and day >=22 and day<=28: return 'THG'\n",
    "    if month == 1: return 'NYR'\n",
    "    return 'NOT'\n",
    "\n",
    "def Season(month,day):\n",
    "    if(month in [9,10,11]) : return 'AUT'\n",
    "    if(month in [12,1,2]) : return 'WIN'\n",
    "    if(month in [3,4,5]) : return 'SPR'\n",
    "    if(month in [6,7,8]) : return 'SUM'\n",
    "    return 'NOT'\n",
    "\n",
    "def train(development):\n",
    "    df_train = pd.read_csv('dataset/train.csv')\n",
    "    y_train = df_train[['PAX']]\n",
    "    y_test = None\n",
    "    \n",
    "    if(development==False):\n",
    "        df_test = pd.read_csv('dataset/test.csv')\n",
    "    else:\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        df_train, df_test, y_train, y_test = train_test_split(df_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "    ### Extract the date and add new features from date\n",
    "\n",
    "    # TRAIN SET\n",
    "    tfdates = df_train.apply(lambda row: transform_date(row['DateOfDeparture']), axis=1)\n",
    "    years = [t[0] for t in tfdates]\n",
    "    months = [t[1] for t in tfdates]\n",
    "    days = [t[2] for t in tfdates]\n",
    "    weekdays = [t[3] for t in tfdates]\n",
    "    df_train['Year'], df_train['Month'],df_train['Day'], df_train['WeekDay'] = years, months, days, weekdays\n",
    "\n",
    "    # TEST SET\n",
    "    tfdates = df_test.apply(lambda row: transform_date(row['DateOfDeparture']), axis=1)\n",
    "    years = [t[0] for t in tfdates]\n",
    "    months = [t[1] for t in tfdates]\n",
    "    days = [t[2] for t in tfdates]\n",
    "    weekdays = [t[3] for t in tfdates]\n",
    "    df_test['Year'], df_test['Month'],df_test['Day'], df_test['WeekDay'] = years, months, days, weekdays\n",
    "\n",
    "    ### Extract the distance from coordinates, longtitude and latitude are inversed -- !!!Dataset's error!!!\n",
    "\n",
    "    # TRAIN SET\n",
    "    distances = df_train.apply(lambda row: round(get_distance_km(row['LongitudeDeparture'],row['LatitudeDeparture'],row['LongitudeArrival'],row['LatitudeArrival']),3), axis=1)\n",
    "    df_train['Distance'] = distances\n",
    "\n",
    "    # TEST SET\n",
    "    distances = df_test.apply(lambda row: round(get_distance_km(row['LongitudeDeparture'],row['LatitudeDeparture'],row['LongitudeArrival'],row['LatitudeArrival']),3), axis=1)\n",
    "    df_test['Distance'] = distances\n",
    "\n",
    "    ### Set min and max weeks to departure\n",
    "\n",
    "    # TRAIN SET\n",
    "    mins = df_train.apply(lambda row: round(row['WeeksToDeparture']-row['std_wtd'],3), axis=1)\n",
    "    maxs = df_train.apply(lambda row: round(row['WeeksToDeparture']+row['std_wtd'],3), axis=1)\n",
    "\n",
    "    df_train['MinWTD'] = mins\n",
    "    df_train['MaxWTD'] = maxs\n",
    "\n",
    "    # TEST SET\n",
    "    mins = df_test.apply(lambda row: round(row['WeeksToDeparture']-row['std_wtd'],3), axis=1)\n",
    "    maxs = df_test.apply(lambda row: round(row['WeeksToDeparture']+row['std_wtd'],3), axis=1)\n",
    "\n",
    "    df_test['MinWTD'] = mins\n",
    "    df_test['MaxWTD'] = maxs\n",
    "\n",
    "    ### Find holidays, seasons\n",
    "\n",
    "    # TRAIN SET\n",
    "    holis = df_train.apply(lambda row: Holiday(row['Month'],row['Day']), axis=1)\n",
    "    seas = df_train.apply(lambda row: Season(row['Month'],row['Day']), axis=1)\n",
    "\n",
    "    df_train['Holiday'] = holis\n",
    "    df_train['Season'] = seas\n",
    "\n",
    "    # TEST SET\n",
    "    holis = df_test.apply(lambda row: Holiday(row['Month'],row['Day']), axis=1)\n",
    "    seas = df_test.apply(lambda row: Season(row['Month'],row['Day']), axis=1)\n",
    "\n",
    "    df_test['Holiday'] = holis\n",
    "    df_test['Season'] = seas\n",
    "\n",
    "    torem = ['DateOfDeparture','CityDeparture','LongitudeDeparture','LatitudeDeparture','CityArrival','LongitudeArrival','LatitudeArrival','WeeksToDeparture','std_wtd','PAX','MinWTD','MaxWTD']\n",
    "    \n",
    "    if(development==False):\n",
    "        \n",
    "        df_train.drop(torem, axis=1, inplace=True)\n",
    "                \n",
    "        torem.remove('PAX')\n",
    "        \n",
    "        df_test.drop(torem, axis=1, inplace=True)\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        df_train.drop(torem, axis=1, inplace=True)\n",
    "        \n",
    "        df_test.drop(torem, axis=1, inplace=True)\n",
    "        \n",
    "    df_train.reset_index(drop=True,inplace=True)\n",
    "    df_test.reset_index(drop=True,inplace=True)\n",
    "        \n",
    "    print(df_train.head(),'\\n'*5)\n",
    "    print(df_test.head())\n",
    "    \n",
    "    return df_train, df_test, y_train, y_test\n",
    "\n",
    "def runModel(df_train, df_test, y_train, showTsne):\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    \n",
    "    le = LabelEncoder()\n",
    "    # Departure and Arrival have the same values so we train only on Departure\n",
    "    le.fit(df_train['Departure'])\n",
    "    df_train['Departure'] = le.transform(df_train['Departure'])\n",
    "    df_train['Arrival'] = le.transform(df_train['Arrival'])\n",
    "    df_test['Departure'] = le.transform(df_test['Departure'])\n",
    "    df_test['Arrival'] = le.transform(df_test['Arrival'])\n",
    "\n",
    "    le.fit(df_train['Year'])\n",
    "    df_train['Year'] = le.transform(df_train['Year'])\n",
    "    le.fit(df_test['Year'])\n",
    "    df_test['Year'] = le.transform(df_test['Year'])\n",
    "    le.fit(df_train['Month'])\n",
    "    df_train['Month'] = le.transform(df_train['Month'])\n",
    "    le.fit(df_test['Month'])\n",
    "    df_test['Month'] = le.transform(df_test['Month'])\n",
    "    le.fit(df_train['Day'])\n",
    "    df_train['Day'] = le.transform(df_train['Day'])\n",
    "    le.fit(df_test['Day'])\n",
    "    df_test['Day'] = le.transform(df_test['Day'])\n",
    "    le.fit(df_train['WeekDay'])\n",
    "    df_train['WeekDay'] = le.transform(df_train['WeekDay'])\n",
    "    le.fit(df_test['WeekDay'])\n",
    "    df_test['WeekDay'] = le.transform(df_test['WeekDay'])\n",
    "\n",
    "    le.fit(df_train['Holiday'])\n",
    "    df_train['Holiday'] = le.transform(df_train['Holiday'])\n",
    "    le.fit(df_test['Holiday'])\n",
    "    df_test['Holiday'] = le.transform(df_test['Holiday'])\n",
    "\n",
    "    le.fit(df_train['Season'])\n",
    "    df_train['Season'] = le.transform(df_train['Season'])\n",
    "    le.fit(df_test['Season'])\n",
    "    df_test['Season'] = le.transform(df_test['Season'])\n",
    "\n",
    "    import numpy as np\n",
    "    import codecs\n",
    "    import keras\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dense, Dropout\n",
    "    from keras.utils import np_utils\n",
    "    from keras import backend as K\n",
    "    from imblearn.over_sampling import RandomOverSampler\n",
    "    from imblearn.under_sampling import TomekLinks\n",
    "    from imblearn.pipeline import make_pipeline\n",
    "    from keras import initializers \n",
    "    import tensorflow as tf\n",
    "    \n",
    "    ### Uncomment for GPU\n",
    "    \n",
    "    #config = tf.ConfigProto(device_count={'GPU':1, 'CPU':56})\n",
    "    #sess = tf.Session(config=config)\n",
    "    #keras.backend.set_session(sess)\n",
    "    \n",
    "    X_train = df_train\n",
    "    X_test = df_test\n",
    "    y_train = np.ravel(y_train)\n",
    "    \n",
    "    ### Scale the data\n",
    "    \n",
    "    from sklearn.preprocessing import minmax_scale\n",
    "    \n",
    "    X_train['Distance'] = minmax_scale(X_train['Distance'])\n",
    "    X_test['Distance'] = minmax_scale(X_test['Distance'])\n",
    "    \n",
    "    # # One-Hot encoding\n",
    "\n",
    "    X_train_dep = pd.get_dummies(X_train['Departure'],prefix='dep')\n",
    "    X_train_arr = pd.get_dummies(X_train['Arrival'],prefix='arr')\n",
    "    X_train_y = pd.get_dummies(X_train['Year'],prefix='y')\n",
    "    X_train_m = pd.get_dummies(X_train['Month'],prefix='m')\n",
    "    X_train_d = pd.get_dummies(X_train['Day'],prefix='d')\n",
    "    X_train_wd = pd.get_dummies(X_train['WeekDay'],prefix='wd')\n",
    "    X_train_hol = pd.get_dummies(X_train['Holiday'],prefix='hol')\n",
    "    X_train_sea = pd.get_dummies(X_train['Season'],prefix='sea')\n",
    "    \n",
    "    X_train_extra1 = pd.concat([X_train_dep, X_train_y, X_train_m, X_train_d, X_train_wd, X_train_hol, X_train_sea],axis=1)\n",
    "\n",
    "    cols = X_train.columns[[0,1,2,3,4,5,7,8]]\n",
    "    X_train[cols] = minmax_scale(X_train[cols])\n",
    "        \n",
    "    X_train_extra = pd.concat([X_train['Departure'], X_train['Arrival'], X_train['Year'], X_train['Month'], X_train['Day'], X_train['WeekDay'], X_train['Holiday'], X_train['Season'], X_train['Distance']],axis=1)\n",
    "    \n",
    "    from sklearn.decomposition import PCA\n",
    "    \n",
    "    pca = PCA(n_components=1)\n",
    "    pca.fit(X_train_extra)\n",
    "    X_train_extra = pca.transform(X_train_extra)\n",
    "    X_train_extra = pd.DataFrame(X_train_extra)\n",
    "    \n",
    "    X_train_extra.reset_index(drop=True,inplace=True)    \n",
    "    \n",
    "    X_train = pd.concat([X_train,X_train_extra,X_train_dep,X_train_arr,X_train_extra1],axis=1,ignore_index=True)\n",
    "\n",
    "    X_train.drop([0,1],axis=1, inplace=True)\n",
    "    \n",
    "    idx_scale = [9]\n",
    "    \n",
    "    for i in idx_scale:\n",
    "        X_train[i] = minmax_scale(X_train[i])\n",
    "    \n",
    "    \n",
    "    X_test_dep = pd.get_dummies(X_test['Departure'],prefix='dep')\n",
    "    X_test_arr = pd.get_dummies(X_test['Arrival'],prefix='arr')\n",
    "    X_test_y = pd.get_dummies(X_test['Year'],prefix='y')\n",
    "    X_test_m = pd.get_dummies(X_test['Month'],prefix='m')\n",
    "    X_test_d = pd.get_dummies(X_test['Day'],prefix='d')\n",
    "    X_test_wd = pd.get_dummies(X_test['WeekDay'],prefix='wd')\n",
    "    X_test_hol = pd.get_dummies(X_test['Holiday'],prefix='hol')\n",
    "    X_test_sea = pd.get_dummies(X_test['Season'],prefix='sea')\n",
    "    \n",
    "    X_test_extra1 = pd.concat([X_test_dep, X_test_y, X_test_m, X_test_d, X_test_wd, X_test_hol, X_test_sea],axis=1)\n",
    "    \n",
    "    cols = X_test.columns[[0,1,2,3,4,5,7,8]]\n",
    "    X_test[cols] = minmax_scale(X_test[cols])\n",
    "        \n",
    "    X_test_extra = pd.concat([X_test['Departure'], X_test['Arrival'], X_test['Year'], X_test['Month'], X_test['Day'], X_test['WeekDay'], X_test['Holiday'], X_test['Season'], X_test['Distance']],axis=1)\n",
    "\n",
    "    X_test_extra = pca.transform(X_test_extra)\n",
    "    X_test_extra = pd.DataFrame(X_test_extra)\n",
    "    \n",
    "    X_test_extra.reset_index(drop=True,inplace=True)\n",
    "\n",
    "    X_test = pd.concat([X_test,X_test_extra,X_test_dep,X_test_arr,X_test_extra1],axis=1, ignore_index=True)\n",
    "    \n",
    "    X_test.drop([0,1],axis=1, inplace=True)\n",
    "    \n",
    "    idx_scale = [9]\n",
    "    for i in idx_scale:\n",
    "        X_test[i] = minmax_scale(X_test[i])\n",
    "        \n",
    "    ## TSNE\n",
    "    \n",
    "    if(showTsne):\n",
    "    \n",
    "        from sklearn.manifold import TSNE \n",
    "        tsne = TSNE(n_components=2,n_iter=250)\n",
    "        tsne_res = tsne.fit_transform(X_train)\n",
    "\n",
    "        df_tnse = pd.DataFrame(tsne_res)\n",
    "\n",
    "        df_pax = pd.DataFrame(y_train)\n",
    "\n",
    "        df_tnse = pd.concat([df_tnse,df_pax],axis=1,ignore_index=True)\n",
    "\n",
    "        df_tnse.columns = ['X','Y','label']\n",
    "\n",
    "        df_tnse['label'] = df_tnse['label'].astype(str)\n",
    "\n",
    "        print(df_tnse.head())\n",
    "\n",
    "        chart = ggplot(df_tnse,aes(x='X',y='Y',color='label'))+ geom_point(alpha=0.8) + ggtitle('tSNE')\n",
    "\n",
    "        chart.show()\n",
    "    \n",
    "    print(X_train.head())\n",
    "    X_train = X_train.values\n",
    "    print(X_train.shape)\n",
    "    \n",
    "    def baseline_model1():\n",
    "        # create model\n",
    "        model = Sequential()\n",
    "        model.add(Dense(units=200, activation='relu' ,kernel_initializer=keras.initializers.he_uniform(seed=None), bias_initializer='ones',input_shape=(X_train.shape[1],)))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(units=100,activation='relu', kernel_initializer=keras.initializers.he_uniform(seed=None), bias_initializer='ones'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(units=50, activation='relu', kernel_initializer=keras.initializers.he_uniform(seed=None), bias_initializer='ones'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(units=25, activation='relu', kernel_initializer=keras.initializers.he_uniform(seed=None),bias_initializer='ones'))\n",
    "        model.add(Dense(8, activation='softmax', kernel_initializer=keras.initializers.he_uniform(seed=None)))\n",
    "        # Compile model\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adagrad')\n",
    "        return model\n",
    "    \n",
    "    def baseline_model2():\n",
    "        # create model\n",
    "        model = Sequential()\n",
    "        model.add(Dense(units=200, activation='relu' ,kernel_initializer=keras.initializers.he_uniform(seed=None), bias_initializer='ones',input_shape=(X_train.shape[1],)))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(units=100,activation='relu', kernel_initializer=keras.initializers.he_uniform(seed=None), bias_initializer='ones'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(units=50, activation='relu', kernel_initializer=keras.initializers.he_uniform(seed=None), bias_initializer='ones'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(units=25, activation='relu', kernel_initializer=keras.initializers.he_uniform(seed=None),bias_initializer='ones'))\n",
    "        model.add(Dense(units=16, activation='relu', kernel_initializer=keras.initializers.he_uniform(seed=None),bias_initializer='ones'))\n",
    "        model.add(Dense(8, activation='softmax', kernel_initializer=keras.initializers.he_uniform(seed=None)))\n",
    "        # Compile model\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adadelta')\n",
    "        return model\n",
    "    \n",
    "    def baseline_model3():\n",
    "        # create model\n",
    "        model = Sequential()\n",
    "        model.add(Dense(units=300, activation='relu' ,kernel_initializer=keras.initializers.he_uniform(seed=None), bias_initializer='ones',input_shape=(X_train.shape[1],)))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(units=100,activation='relu', kernel_initializer=keras.initializers.he_uniform(seed=None), bias_initializer='ones'))\n",
    "        model.add(Dropout(0.4))\n",
    "        model.add(Dense(units=50, activation='relu', kernel_initializer=keras.initializers.he_uniform(seed=None), bias_initializer='ones'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(units=25, activation='relu', kernel_initializer=keras.initializers.he_uniform(seed=None),bias_initializer='ones'))\n",
    "        model.add(Dense(8, activation='softmax', kernel_initializer=keras.initializers.he_uniform(seed=None)))\n",
    "        # Compile model\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='nadam')\n",
    "        return model\n",
    "    \n",
    "    def baseline_model4():\n",
    "        # create model\n",
    "        model = Sequential()\n",
    "        model.add(Dense(units=200, activation='relu' ,kernel_initializer=keras.initializers.he_uniform(seed=None), bias_initializer='zeros',input_shape=(X_train.shape[1],)))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(units=100,activation='relu', kernel_initializer=keras.initializers.he_uniform(seed=None), bias_initializer='zeros'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(units=50, activation='relu', kernel_initializer=keras.initializers.he_uniform(seed=None), bias_initializer='zeros'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(units=25, activation='relu', kernel_initializer=keras.initializers.he_uniform(seed=None),bias_initializer='zeros'))\n",
    "        model.add(Dense(units=16, activation='relu', kernel_initializer=keras.initializers.he_uniform(seed=None),bias_initializer='zeros'))\n",
    "        #sto apo panw eixe ones\n",
    "        model.add(Dense(8, activation='softmax', kernel_initializer=keras.initializers.he_uniform(seed=None)))\n",
    "        # Compile model\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "        return model\n",
    "    \n",
    "    def baseline_model5():\n",
    "        # create model\n",
    "        model = Sequential()\n",
    "        model.add(Dense(units=200, activation='relu' ,kernel_initializer=keras.initializers.he_uniform(seed=None), bias_initializer='zeros',input_shape=(X_train.shape[1],)))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(units=100,activation='relu', kernel_initializer=keras.initializers.he_uniform(seed=None), bias_initializer='zeros'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(units=50, activation='relu', kernel_initializer=keras.initializers.he_uniform(seed=None), bias_initializer='zeros'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(units=25, activation='relu', kernel_initializer=keras.initializers.he_uniform(seed=None),bias_initializer='zeros'))\n",
    "        model.add(Dense(8, activation='softmax', kernel_initializer=keras.initializers.he_uniform(seed=None)))\n",
    "        # Compile model\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "        return model\n",
    "    \n",
    "    def baseline_model6():\n",
    "        # create model\n",
    "        model = Sequential()\n",
    "        model.add(Dense(units=300, activation='relu' ,kernel_initializer=keras.initializers.he_uniform(seed=None), bias_initializer='ones',input_shape=(X_train.shape[1],)))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(units=100,activation='relu', kernel_initializer=keras.initializers.he_uniform(seed=None), bias_initializer='ones'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(units=50, activation='relu', kernel_initializer=keras.initializers.he_uniform(seed=None), bias_initializer='ones'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(units=25, activation='relu', kernel_initializer=keras.initializers.he_uniform(seed=None),bias_initializer='ones'))\n",
    "        model.add(Dense(8, activation='softmax', kernel_initializer=keras.initializers.he_uniform(seed=None)))\n",
    "        # Compile model\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='nadam')\n",
    "        return model\n",
    "    \n",
    "\n",
    "    sampling_strategy = 'minority'\n",
    "    \n",
    "    tl = RandomOverSampler(sampling_strategy)\n",
    "    X_train, y_train = tl.fit_resample(X_train, y_train)\n",
    "    y_train = np_utils.to_categorical(y_train)\n",
    "   \n",
    "    model1 = baseline_model1()\n",
    "    model2 = baseline_model2()\n",
    "    model3 = baseline_model3()\n",
    "    model4 = baseline_model4()\n",
    "    model5 = baseline_model5()\n",
    "    model6 = baseline_model6()\n",
    "    model1.fit(X_train, y_train, epochs=150, batch_size=256,shuffle=True)\n",
    "    model2.fit(X_train, y_train, epochs=150, batch_size=256,shuffle=True)\n",
    "    model3.fit(X_train, y_train, epochs=100, batch_size=256,shuffle=True)\n",
    "    model4.fit(X_train, y_train, epochs=150, batch_size=256,shuffle=True)\n",
    "    model5.fit(X_train, y_train, epochs=100, batch_size=256,shuffle=True)\n",
    "    model6.fit(X_train, y_train, epochs=100, batch_size=256,shuffle=True)\n",
    "    prob1 = model1.predict(X_test)\n",
    "    prob2 = model2.predict(X_test)\n",
    "    prob3 = model3.predict(X_test)\n",
    "    prob4 = model4.predict(X_test)\n",
    "    prob5 = model5.predict(X_test)\n",
    "    prob6 = model6.predict(X_test)\n",
    "\n",
    "    final_prob = (prob1 + prob2 + prob3 + prob4 + prob5 + prob6) / 6\n",
    "    \n",
    "    return final_prob.argmax(axis=1)\n",
    "    \n",
    "def evaluateModel(y_pred, y_test, development):\n",
    "    if(development==False):\n",
    "        # Write the predictions to the file\n",
    "        import csv\n",
    "        with open('y_pred.csv', 'w', newline='') as csvfile:\n",
    "            writer = csv.writer(csvfile, delimiter=',')\n",
    "            writer.writerow(['Id', 'Label'])\n",
    "            for i in range(y_pred.shape[0]):\n",
    "                writer.writerow([i, y_pred[i]])\n",
    "        return None\n",
    "    else:\n",
    "        # FOR DEVELOPMENT SET\n",
    "        print(\"Your predictions: \", y_pred)\n",
    "        # print(\"Test predictions: \", y_test)\n",
    "        from sklearn.metrics import f1_score\n",
    "        return f1_score(y_test, y_pred, average='micro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6698,
     "status": "ok",
     "timestamp": 1546696275806,
     "user": {
      "displayName": "Petros Stavropoulos",
      "photoUrl": "https://lh6.googleusercontent.com/-0wuNytUFemY/AAAAAAAAAAI/AAAAAAAAHCA/dkKAx3t2kiE/s64/photo.jpg",
      "userId": "01438099347210539127"
     },
     "user_tz": -120
    },
    "id": "JGdZM2wraoNy",
    "outputId": "861cfcee-d171-42c4-a01d-64f68e2e85d7"
   },
   "outputs": [],
   "source": [
    "development = False\n",
    "\n",
    "df_train, df_test, y_train, y_test = train(development)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 25840
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 304116,
     "status": "ok",
     "timestamp": 1546696575000,
     "user": {
      "displayName": "Petros Stavropoulos",
      "photoUrl": "https://lh6.googleusercontent.com/-0wuNytUFemY/AAAAAAAAAAI/AAAAAAAAHCA/dkKAx3t2kiE/s64/photo.jpg",
      "userId": "01438099347210539127"
     },
     "user_tz": -120
    },
    "id": "K7BrXLCVaoN3",
    "outputId": "4fca5946-5924-4fcb-da12-fd96724b0936"
   },
   "outputs": [],
   "source": [
    "showTsne = False\n",
    "y_pred = runModel(df_train, df_test, y_train, showTsne)\n",
    "score = evaluateModel(y_pred, y_test, development)\n",
    "if(score != None): print(score)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Project_Report_Keras_67215.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
